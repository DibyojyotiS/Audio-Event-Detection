{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6-final"
    },
    "orig_nbformat": 2,
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "colab": {
      "name": "task2_code.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CrXrYzTjpUEQ",
        "outputId": "5403abb4-38a2-4cab-a2ed-067c30f827e3"
      },
      "source": [
        "!nvidia-smi\n",
        "from google.colab import drive # run this only on colab\n",
        "drive.mount('/content/gdrive')\n",
        "%cd /content/gdrive/MyDrive/Jellyfish/code_and_data/\n",
        "%ls\n",
        "%tensorflow_version 2.x"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Thu Dec 17 08:09:25 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 455.45.01    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   40C    P8    10W /  70W |      0MiB / 15079MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n",
            "Mounted at /content/gdrive\n",
            "/content/gdrive/MyDrive/Jellyfish/code_and_data\n",
            " bettersplit.py        helper_funcs.py        \u001b[0m\u001b[01;34m__pycache__\u001b[0m/      task1.ipynb\n",
            " \u001b[01;34mcellar\u001b[0m/               hsweeptrain_task1.py   randsplit.py      task2.ipynb\n",
            " extract_features.py   jellyGPU.ipynb        \u001b[01;34m'#shared_train'\u001b[0m/   utils.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7K7kCLVupUEX"
      },
      "source": [
        "# Main code starts here"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gb0j9oxmpUEX"
      },
      "source": [
        "import os\n",
        "import librosa\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, Model, optimizers, callbacks"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aWZckhxbpUEY"
      },
      "source": [
        "## Make the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-EKtLk_dpUEY"
      },
      "source": [
        "# load class data\n",
        "files_n_classes = np.loadtxt(\n",
        "    fname= \"#shared_train/labels_train.csv\",\n",
        "    delimiter=\",\",\n",
        "    dtype='S',\n",
        "    skiprows=1\n",
        ").astype(str)\n",
        "for i in range(len(files_n_classes)):\n",
        "    files_n_classes[i][0] = files_n_classes[i][0].split('.')[0]\n",
        "\n",
        "unique_classes, counts = np.unique(files_n_classes[:,1], return_counts=True)\n",
        "\n",
        "# seperate the files into test, dev and train to avoid data repetation, load sets\n",
        "test_files_n_classes = np.loadtxt(\"cellar/test_files_n_classes.txt\", dtype=str)\n",
        "train_files_n_classes = np.loadtxt(\"cellar/train_files_n_classes.txt\", dtype=str)\n",
        "dev_files_n_classes = np.loadtxt(\"cellar/dev_files_n_classes.txt\", dtype=str)\n",
        "\n",
        "# generate train, dev and test sets by cropping concatenating given files\n",
        "num_train = 1024\n",
        "num_dev = 128\n",
        "num_test = 128\n",
        "def generate_rnd_seq(choices, files_n_classes, min_classes, max_classes, use_rndcrop=False):\n",
        "    history = {'rnd_file':[], 'crop_start':[], 'n_timesteps':[]}\n",
        "    raw_seq = np.array([])\n",
        "    seq_spec = []\n",
        "    while len(np.unique(raw_seq)) < min_classes: \n",
        "        # rnd_size = np.random.randint(min_classes, max_classes)\n",
        "        rnd_size = max_classes\n",
        "        raw_seq = np.random.choice(np.arange(len(choices)), size= rnd_size)\n",
        "    for event_idx in raw_seq:\n",
        "        event_type = choices[event_idx]\n",
        "        event_example_files = np.squeeze(files_n_classes[:,0][np.argwhere(files_n_classes[:,1] == event_type)])\n",
        "        rnd_file = np.random.choice(event_example_files)\n",
        "        spec = np.load(f\"cellar/spectograms/{rnd_file}.npy\")\n",
        "        if use_rndcrop:\n",
        "            n_timesteps = np.random.randint(6, spec.shape[1]+1) #if spec.shape[1]>6 else spec.shape[1]\n",
        "            crop_start = np.random.randint(0, spec.shape[1] - n_timesteps + 1) \n",
        "            cropped_spec = spec[:, crop_start: crop_start + n_timesteps]\n",
        "            seq_spec.append(cropped_spec)\n",
        "        else:\n",
        "            crop_start = 0\n",
        "            n_timesteps = spec.shape[1]\n",
        "            seq_spec.append(spec)\n",
        "        history['rnd_file'].append(rnd_file)\n",
        "        history['crop_start'].append(crop_start)\n",
        "        history['n_timesteps'].append(n_timesteps)\n",
        "    seq = raw_seq[np.insert(np.diff(raw_seq).astype(bool), 0, True)]\n",
        "    noncrepeat_seq = [choices[idx] for idx in seq]\n",
        "    raw_seq = [choices[idx] for idx in raw_seq]\n",
        "    spec_seq = np.concatenate(seq_spec, 1)\n",
        "    history['raw_seq'] = raw_seq\n",
        "    history['noncrepeat_seq'] = noncrepeat_seq\n",
        "    return spec_seq, np.asarray(noncrepeat_seq), np.asarray(raw_seq), history\n",
        "\n",
        "def mkdirs(string):\n",
        "    if not os.path.exists(f\"cellar/task2_sequences/seq_spectograms/{string}\"): os.makedirs(f\"cellar/task2_sequences/seq_spectograms/{string}\")\n",
        "    if not os.path.exists(f\"cellar/task2_sequences/labels_noncrepeat/{string}\"): os.makedirs(f\"cellar/task2_sequences/labels_noncrepeat/{string}\")\n",
        "    if not os.path.exists(f\"cellar/task2_sequences/labels_raw/{string}\"): os.makedirs(f\"cellar/task2_sequences/labels_raw/{string}\")\n",
        "    if not os.path.exists(f\"cellar/task2_sequences/history/{string}\"): os.makedirs(f\"cellar/task2_sequences/history/{string}\")\n",
        "\n",
        "def makeit(string, num, unique_classes, files_n_classes):\n",
        "    dataset = []\n",
        "    class_map = {uniq: i for i, uniq in enumerate(unique_classes)}\n",
        "    for i in range(num):\n",
        "        spec_seq, noncrepeat_seq, raw_seq, hist = generate_rnd_seq(unique_classes, files_n_classes, 1, 5)\n",
        "        noncrepeat_idx_seq = [class_map[event_name] for event_name in noncrepeat_seq]\n",
        "        raw_idx_seq = [class_map[event_name] for event_name in raw_seq]\n",
        "        np.save(f\"cellar/task2_sequences/seq_spectograms/{string}/{i}.npy\", spec_seq)\n",
        "        np.savetxt(f\"cellar/task2_sequences/labels_noncrepeat/{string}/{i}.txt\", noncrepeat_seq, fmt=\"%s\")\n",
        "        np.savetxt(f\"cellar/task2_sequences/labels_raw/{string}/{i}.txt\", raw_seq, fmt=\"%s\")\n",
        "        np.savetxt(f\"cellar/task2_sequences/history/{string}/{i}.txt\", [hist], fmt='%s')\n",
        "        dataset.append([f\"cellar/task2_sequences/seq_spectograms/{string}/{i}.npy\", noncrepeat_idx_seq, raw_idx_seq, noncrepeat_seq, raw_seq])\n",
        "    np.save(f\"cellar/task2_sequences/{string}_set.npy\", np.asarray(dataset))\n",
        "    print(f\" made {string}\")\n",
        "    return np.asarray(dataset)\n",
        "\n",
        "if os.path.isfile(\"cellar/task2_sequences/train_set.npy\") and \\\n",
        "    os.path.isfile(\"cellar/task2_sequences/dev_set.npy\") and \\\n",
        "    os.path.isfile(\"cellar/task2_sequences/test_set.npy\"):\n",
        "    train_set = np.load(\"cellar/task2_sequences/train_set.npy\", allow_pickle=True)\n",
        "    dev_set = np.load(\"cellar/task2_sequences/dev_set.npy\", allow_pickle=True)\n",
        "    test_set = np.load(\"cellar/task2_sequences/test_set.npy\", allow_pickle=True)\n",
        "else:\n",
        "    print(\"making datasets\")\n",
        "    mkdirs(\"train\"); mkdirs(\"dev\"); mkdirs(\"test\")\n",
        "    train_set = makeit(\"train\", num_train, unique_classes, train_files_n_classes)\n",
        "    dev_set = makeit(\"dev\", num_dev, unique_classes, dev_files_n_classes)\n",
        "    test_set = makeit(\"test\", num_test, unique_classes, test_files_n_classes)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ToRpL43TpUEZ"
      },
      "source": [
        "## TRAINING"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dTGUqAUOpUEa"
      },
      "source": [
        "### model for training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CwCmo4j1sKcZ"
      },
      "source": [
        "blank_label = 10 # label denoting blank\r\n",
        "freq_dim = 52"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dn7Lzt3FpUEa"
      },
      "source": [
        "# ctc loss layer\n",
        "class ctclosslayer(layers.Layer):\n",
        "    def __init__(self, trainable=True, name=None, dtype=None, dynamic=False, **kwargs):\n",
        "        super(ctclosslayer, self).__init__(trainable=trainable, name=name, dtype=dtype, dynamic=dynamic, **kwargs)\n",
        "\n",
        "    def call(self, logits, labels, **kwargs):\n",
        "        considered_logits = tf.identity(logits[:, 2:, :]) \n",
        "\n",
        "        batch_size = tf.shape(labels)[0]\n",
        "        label_length = tf.shape(labels)[1] * tf.ones((batch_size, 1), tf.int32)\n",
        "        logit_length = tf.shape(considered_logits)[1] * tf.ones((batch_size, 1), tf.int32)\n",
        "        ctcloss = tf.keras.backend.ctc_batch_cost(labels, considered_logits, logit_length, label_length)\n",
        "        self.add_loss(tf.reduce_mean(ctcloss))\n",
        "\n",
        "        decoded_labels = tf.where(tf.equal(labels, blank_label), -1, labels)\n",
        "        decoded_logits = tf.keras.backend.ctc_decode(considered_logits, tf.squeeze(logit_length))[0][0][:, :tf.shape(labels)[1]]\n",
        "        editd = tf.edit_distance(tf.sparse.from_dense(decoded_logits), tf.sparse.from_dense(tf.cast(decoded_labels, tf.int64)), normalize=True)\n",
        "        self.add_metric(tf.reduce_mean(editd), aggregation='mean', name=\"edit_distance\")\n",
        "\n",
        "        return logits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dyw_5GMIpUEa",
        "outputId": "1e8caf4f-79a5-45de-de8d-5fbb30d1537a"
      },
      "source": [
        "# model for training\n",
        "sif_input = layers.Input((100, freq_dim, 1), name= 'sif_input')\n",
        "target_labels = layers.Input((None,), dtype=tf.int32, name= 'target_labels')\n",
        "\n",
        "conv1 = layers.Conv2D(128, (3,3), activation='relu', padding='same')(sif_input)\n",
        "bnconv11 = layers.BatchNormalization()(conv1)\n",
        "mp1 = layers.MaxPooling2D((1,3))(bnconv11)\n",
        "mp1 = layers.Dropout(0.5)(mp1)\n",
        "\n",
        "conv2 = layers.Conv2D(128, (3,3), activation='relu', padding='same')(mp1)\n",
        "bnconv2 = layers.BatchNormalization()(conv2)\n",
        "mp2 = layers.MaxPooling2D((1,2))(bnconv2)\n",
        "mp2 = layers.Dropout(0.5)(mp2)\n",
        "\n",
        "conv3 = layers.Conv2D(128, (3,3), activation='relu', padding='same')(mp2)\n",
        "bnconv3 = layers.BatchNormalization()(conv3)\n",
        "mp3 = layers.MaxPooling2D((1,2))(bnconv3)\n",
        "mp3 = layers.Dropout(0.5)(mp3)\n",
        "\n",
        "conv4 = layers.Conv2D(128, (3,3), activation='relu', padding='same')(mp3)\n",
        "bnconv4 = layers.BatchNormalization()(conv4)\n",
        "mp4 = layers.MaxPooling2D((1,2))(bnconv4)\n",
        "mp4 = layers.Dropout(0.5)(mp4)\n",
        "\n",
        "conv_output = layers.Reshape((-1, mp4.shape[2]*mp4.shape[3]))(mp4)\n",
        "conv_output = layers.MaxPool1D(pool_size=25, strides=15)(conv_output)\n",
        "\n",
        "lstm_seq = layers.Bidirectional(layers.LSTM(64, return_sequences=True))(conv_output)\n",
        "lstm_seq = layers.Dropout(0.5)(lstm_seq)\n",
        "lstm_seq = layers.Bidirectional(layers.LSTM(64, return_sequences=True))(lstm_seq)\n",
        "lstm_seq = layers.Dropout(0.5)(lstm_seq)\n",
        "\n",
        "out_seq = layers.Dense(len(unique_classes)+1, activation='softmax', name='output')(lstm_seq)\n",
        "out_seq = ctclosslayer()(out_seq, target_labels)\n",
        "\n",
        "model = Model([sif_input, target_labels], out_seq)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From D:\\anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:201: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UjGu6q3ubLa7"
      },
      "source": [
        "model.compile(optimizers.Adam(lr=0.005, clipnorm=5.0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_5e1ZqQKrlBA",
        "outputId": "9c58e743-53d5-44d5-f637-ee401b1514ab"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"functional_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "sif_input (InputLayer)          [(None, 100, 52, 1)] 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d (Conv2D)                 (None, 100, 52, 128) 1280        sif_input[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization (BatchNorma (None, 100, 52, 128) 512         conv2d[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D)    (None, 100, 17, 128) 0           batch_normalization[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "dropout (Dropout)               (None, 100, 17, 128) 0           max_pooling2d[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 100, 17, 128) 147584      dropout[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 100, 17, 128) 512         conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2D)  (None, 100, 8, 128)  0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 100, 8, 128)  0           max_pooling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 100, 8, 128)  147584      dropout_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 100, 8, 128)  512         conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2D)  (None, 100, 4, 128)  0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_2 (Dropout)             (None, 100, 4, 128)  0           max_pooling2d_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 100, 4, 128)  147584      dropout_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 100, 4, 128)  512         conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2D)  (None, 100, 2, 128)  0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_3 (Dropout)             (None, 100, 2, 128)  0           max_pooling2d_3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "reshape (Reshape)               (None, 100, 256)     0           dropout_3[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d (MaxPooling1D)    (None, 6, 256)       0           reshape[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional (Bidirectional)   (None, 6, 128)       164352      max_pooling1d[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_4 (Dropout)             (None, 6, 128)       0           bidirectional[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_1 (Bidirectional) (None, 6, 128)       98816       dropout_4[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_5 (Dropout)             (None, 6, 128)       0           bidirectional_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "output (Dense)                  (None, 6, 11)        1419        dropout_5[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "target_labels (InputLayer)      [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "ctclosslayer (ctclosslayer)     (None, 6, 11)        0           output[0][0]                     \n",
            "                                                                 target_labels[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 710,667\n",
            "Trainable params: 709,643\n",
            "Non-trainable params: 1,024\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lFCnPxn6pUEb"
      },
      "source": [
        "# train the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lIifastLe0OX"
      },
      "source": [
        "### functions for extracting features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lEc42Yape4Zj"
      },
      "source": [
        "def extract_SIF(raw_spec, W=13, frequency_stride=10):\r\n",
        "    \"\"\"\r\n",
        "    raw_spec: spectogram of shape (Fbins, time)\r\n",
        "    Fbins: number of freq bins\r\n",
        "    returns: array of shape (F, time)\r\n",
        "    \"\"\"\r\n",
        "    # frequency downsampling into F bins \r\n",
        "    L = raw_spec.shape[0]\r\n",
        "    F = (L-W)//frequency_stride + 1\r\n",
        "    SIF = np.zeros((F, raw_spec.shape[1]))\r\n",
        "    for i in range(F):\r\n",
        "        freq_window = raw_spec[i*frequency_stride:i*frequency_stride+W]\r\n",
        "        SIF[i][:] = np.mean(freq_window, 0)    \r\n",
        "    # denoise\r\n",
        "    SIF_dn = SIF - np.min(SIF, axis=0)\r\n",
        "    # augment SIF_dn append per frame time tomain energy\r\n",
        "    energy_shorttime = np.sum(SIF_dn, axis=0)\r\n",
        "    SIF_aug = np.concatenate(\r\n",
        "        [SIF_dn, np.expand_dims(energy_shorttime, 0)], axis= 0)\r\n",
        "\r\n",
        "    return SIF_aug\r\n",
        "\r\n",
        "\r\n",
        "def extract_mbe(spec, sr=44100, n_fft=1024, n_mels=40):\r\n",
        "    # log mel band energies\r\n",
        "    mel_basis = librosa.filters.mel(sr=sr, n_fft=n_fft, n_mels=n_mels)\r\n",
        "    return np.log(np.dot(mel_basis, spec) + 1e-8)\r\n",
        "\r\n",
        "\r\n",
        "def extract_melspec(spec, n_mels = 64):\r\n",
        "    D = spec**2\r\n",
        "    S = librosa.feature.melspectrogram(S=D, sr=44100, n_mels= n_mels)\r\n",
        "    return S\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pUIRDkflfDhs"
      },
      "source": [
        "### data generator class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r4vqC-W5fFxA"
      },
      "source": [
        "# the SIF generator for task 2\r\n",
        "class task2_SIF_generator():\r\n",
        "    def __init__(self, spec_files, labels, blank_label, batch_size, not_infinite, feature='sif', pad_mode='constant', pad_labels=False):\r\n",
        "        \"\"\"\r\n",
        "        spec_files: filepaths\r\n",
        "        labels: list or np array\r\n",
        "        not_infinite: set True is generator should terminate\r\n",
        "        feature in ['sif', 'mbe', 'melspec']\r\n",
        "        \"\"\"\r\n",
        "        self.count = 0\r\n",
        "        self.spec_files = spec_files\r\n",
        "        self.maxcount = len(spec_files)\r\n",
        "        self.labels = labels\r\n",
        "        self.blank_label = blank_label\r\n",
        "        self.batch_size = batch_size\r\n",
        "        self.not_infinite = not_infinite\r\n",
        "        self.continue_generation = True\r\n",
        "        self.pad_labels = pad_labels\r\n",
        "        self.pmode = pad_mode\r\n",
        "        self.feature = feature\r\n",
        "\r\n",
        "        if pad_mode == 'constant':\r\n",
        "            self.pad_fn = lambda label, pad_len: np.pad(label, [0, pad_len], constant_values= self.blank_label, mode=self.pmode)\r\n",
        "        elif pad_mode == 'edge':\r\n",
        "            self.pad_fn = lambda label, pad_len: np.pad(label, [0, pad_len], mode=self.pmode)\r\n",
        "        else:\r\n",
        "            self.pad_fn = lambda label, pad_len: np.pad(label, [0, pad_len], constant_values= self.blank_label, mode=self.pmode)\r\n",
        "\r\n",
        "    def make_batch(self):\r\n",
        "        self.count = self.count % self.maxcount\r\n",
        "        MAX = self.count + self.batch_size\r\n",
        "\r\n",
        "        sifs = []\r\n",
        "        batch_labels = []\r\n",
        "        max_spec_length = 0\r\n",
        "        max_label_len = 0\r\n",
        "        for i in range(self.count, MAX):\r\n",
        "\r\n",
        "            idx = i % self.maxcount\r\n",
        "            raw_spec = np.load(self.spec_files[idx], allow_pickle=True)\r\n",
        "            max_spec_length = max(max_spec_length, raw_spec.shape[1])\r\n",
        "            max_label_len = max(max_label_len, len(self.labels[idx]))\r\n",
        "\r\n",
        "            batch_labels.append(np.asarray(self.labels[idx], np.int32))\r\n",
        "            sifs.append(extract_mbe(raw_spec) if self.feature=='mbe' \\\r\n",
        "                        else extract_SIF(raw_spec) if self.feature=='sif' \\\r\n",
        "                        else extract_melspec(raw_spec))\r\n",
        "\r\n",
        "            # terminate for non-infinite\r\n",
        "            if self.not_infinite and i >= self.maxcount-1: \r\n",
        "                self.continue_generation = False\r\n",
        "                break\r\n",
        "            \r\n",
        "        self.count += self.batch_size\r\n",
        "        \r\n",
        "        for i in range(len(sifs)):\r\n",
        "            sif = sifs[i]\r\n",
        "            pad_len = max_spec_length - sif.shape[1]\r\n",
        "            sifs[i] = np.pad(sif, ((0,0),(0,pad_len))).T\r\n",
        "\r\n",
        "        if self.pad_labels:\r\n",
        "            for i in range(len(batch_labels)):\r\n",
        "                label = batch_labels[i]\r\n",
        "                pad_len = max_label_len - len(label)\r\n",
        "                batch_labels[i] = self.pad_fn(label, pad_len)\r\n",
        "            batch_labels = np.asarray(batch_labels, np.int32)\r\n",
        "\r\n",
        "        sifs = np.asarray(sifs, np.float32)\r\n",
        "\r\n",
        "        return {'sif_input':sifs, 'target_labels':batch_labels},\r\n",
        "        \r\n",
        "\r\n",
        "    def generator(self):\r\n",
        "        '''\r\n",
        "        files: np Array with file names as byte-strings\r\n",
        "        labels: integer labels\r\n",
        "        Output: np array of spectrograms, corresponding labels as a list\r\n",
        "        '''\r\n",
        "        while self.continue_generation:\r\n",
        "            yield self.make_batch()\r\n",
        "\r\n",
        "    def reset(self):\r\n",
        "        self.count = 0\r\n",
        "        self.continue_generation = True\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BOFS-ZtDfq32"
      },
      "source": [
        "### build the test train and dev data generators"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5NC8PxsIpUEZ"
      },
      "source": [
        "# data generators to load batches, labels are non-consequtive-repeting\n",
        "batch_size = 32\n",
        "feature = 'sif'\n",
        "\n",
        "train_datgenerator = task2_SIF_generator(\n",
        "    spec_files= train_set[:,0],\n",
        "    labels= train_set[:,1],\n",
        "    blank_label= blank_label,\n",
        "    feature= feature,\n",
        "    pad_mode= 'constant',#'edge',\n",
        "    batch_size= batch_size,\n",
        "    not_infinite= False,\n",
        "    pad_labels= True\n",
        ")\n",
        "\n",
        "dev_datgenerator = task2_SIF_generator(\n",
        "    spec_files= dev_set[:,0],\n",
        "    labels= dev_set[:,1],\n",
        "    blank_label= blank_label,\n",
        "    feature= feature,\n",
        "    pad_mode= 'constant',#'edge',\n",
        "    batch_size= batch_size,\n",
        "    not_infinite= False,\n",
        "    pad_labels= True\n",
        ")\n",
        "\n",
        "test_datgenerator = task2_SIF_generator(\n",
        "    spec_files= test_set[:,0],\n",
        "    labels= test_set[:,1],\n",
        "    blank_label= blank_label,\n",
        "    feature= feature,\n",
        "    pad_mode= 'constant',#'edge',\n",
        "    batch_size= batch_size,\n",
        "    not_infinite= True,\n",
        "    pad_labels= True\n",
        ")\n",
        "\n",
        "train_datgenerator_tf = tf.data.Dataset.from_generator(\n",
        "    train_datgenerator.generator,\n",
        "    ({'sif_input':tf.float32, 'target_labels':tf.int32},), \n",
        "    output_shapes= ({'sif_input':tf.TensorShape((None, None, freq_dim)), 'target_labels':tf.TensorShape((None, None))},)\n",
        "    ).prefetch(tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "dev_datgenerator_tf = tf.data.Dataset.from_generator(\n",
        "    dev_datgenerator.generator,\n",
        "    ({'sif_input':tf.float32, 'target_labels':tf.int32},), \n",
        "    output_shapes= ({'sif_input':tf.TensorShape((None, None, freq_dim)), 'target_labels':tf.TensorShape((None, None))},)\n",
        "    ).prefetch(tf.data.experimental.AUTOTUNE)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Px4o-54Mb24i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02d0660c-6b4d-4cc5-9c83-9596b9e5456e"
      },
      "source": [
        "tf_log_num = \"sif/1.1\" \n",
        "if os.path.exists(f\"cellar/run/task2/{tf_log_num}/ckeckpoints/best_val\"): \n",
        "    print(\"log dir exists\")\n",
        "    model = tf.keras.models.load_model(f\"cellar/run/task2/{tf_log_num}/ckeckpoints/best_val\")\n",
        "    model.summary()\n",
        "else: \n",
        "    if not os.path.exists(f'cellar/run/task2/{tf_log_num}'): os.makedirs(f'cellar/run/task2/{tf_log_num}')\n",
        "    tf.keras.utils.plot_model(model, to_file=f'cellar/run/task2/{tf_log_num}/model.png', show_shapes=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "log dir exists\n",
            "Model: \"functional_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "sif_input (InputLayer)          [(None, 100, 52, 1)] 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d (Conv2D)                 (None, 100, 52, 128) 1280        sif_input[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization (BatchNorma (None, 100, 52, 128) 512         conv2d[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D)    (None, 100, 17, 128) 0           batch_normalization[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "dropout (Dropout)               (None, 100, 17, 128) 0           max_pooling2d[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 100, 17, 128) 147584      dropout[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 100, 17, 128) 512         conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2D)  (None, 100, 8, 128)  0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 100, 8, 128)  0           max_pooling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 100, 8, 128)  147584      dropout_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 100, 8, 128)  512         conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2D)  (None, 100, 4, 128)  0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_2 (Dropout)             (None, 100, 4, 128)  0           max_pooling2d_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 100, 4, 128)  147584      dropout_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 100, 4, 128)  512         conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2D)  (None, 100, 2, 128)  0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_3 (Dropout)             (None, 100, 2, 128)  0           max_pooling2d_3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "reshape (Reshape)               (None, 100, 256)     0           dropout_3[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d (MaxPooling1D)    (None, 6, 256)       0           reshape[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional (Bidirectional)   (None, 6, 128)       164352      max_pooling1d[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_4 (Dropout)             (None, 6, 128)       0           bidirectional[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_1 (Bidirectional) (None, 6, 128)       98816       dropout_4[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_5 (Dropout)             (None, 6, 128)       0           bidirectional_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "output (Dense)                  (None, 6, 11)        1419        dropout_5[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "target_labels (InputLayer)      [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "ctclosslayer (ctclosslayer)     (None, 6, 11)        0           output[0][0]                     \n",
            "                                                                 target_labels[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 710,667\n",
            "Trainable params: 709,643\n",
            "Non-trainable params: 1,024\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d89FTBR2pUEb"
      },
      "source": [
        "# train the model with ctc loss change tf_log_num for every new model\n",
        "print(\"batch_size: \", batch_size, \"lr: \", model.optimizer.learning_rate.numpy())\n",
        "history = model.fit(train_datgenerator_tf,\n",
        "        epochs= 500,\n",
        "        validation_data= dev_datgenerator_tf,\n",
        "        steps_per_epoch= len(train_set)//batch_size,\n",
        "        validation_steps= len(dev_set)//batch_size,\n",
        "        callbacks=[\n",
        "            callbacks.TensorBoard(log_dir=f'cellar/run/task2/{tf_log_num}/logs', \n",
        "                                histogram_freq=8, write_images=True),\n",
        "            callbacks.ModelCheckpoint(f\"cellar/run/task2/{tf_log_num}/ckeckpoints/best_val\", \n",
        "                                monitor=\"val_edit_distance\", verbose=1, save_best_only=True, period=10),\n",
        "            callbacks.ModelCheckpoint(f\"cellar/run/task2/{tf_log_num}/ckeckpoints/best_train\", \n",
        "                                monitor='edit_distance', verbose=1, save_best_only=True, period=10),\n",
        "            callbacks.ReduceLROnPlateau(monitor='loss', factor=0.1, verbose=1, min_delta=0.001),\n",
        "            callbacks.LambdaCallback(on_epoch_end=lambda b,l: tf.print(f\" - lr: {model.optimizer.learning_rate.numpy()} - \")),\n",
        "            # callbacks.EarlyStopping(patience=10, restore_best_weights=True),\n",
        "        ]\n",
        "        )\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ovzZp0LjpUEc"
      },
      "source": [
        "# save model and save model weights\n",
        "model.save(filepath=f\"cellar/run/task2/{tf_log_num}/model_save.h5\")\n",
        "model.save_weights(filepath=f\"cellar/run/task2/{tf_log_num}/model_weights.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YN7AgJmPpUEc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "254538c4-2882-4754-857f-25eaff28b237"
      },
      "source": [
        "# evaluate on test set\n",
        "test_datgenerator.reset()\n",
        "model.evaluate(test_datgenerator.generator())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4/4 [==============================] - 28s 7s/step - loss: 6.8098 - edit_distance: 0.3306\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[6.809807300567627, 0.33059895038604736]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WjaPG1sFBPhQ",
        "outputId": "323e383d-3a55-43af-8daf-145bbbf8b92e"
      },
      "source": [
        "# evaluate on dev set\r\n",
        "dev_datgenerator.not_infinite = True\r\n",
        "dev_datgenerator.reset()\r\n",
        "model.evaluate(dev_datgenerator.generator())\r\n",
        "dev_datgenerator.not_infinite = False"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4/4 [==============================] - 33s 8s/step - loss: 5.6092 - edit_distance: 0.2365\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "707Zs7B-BQct",
        "outputId": "fc2bec71-11e8-40ae-df02-9e9a9e0786e5"
      },
      "source": [
        "# evaluate on train set\r\n",
        "train_datgenerator.not_infinite = True\r\n",
        "train_datgenerator.reset()\r\n",
        "model.evaluate(train_datgenerator.generator())\r\n",
        "train_datgenerator.not_infinite = False"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "32/32 [==============================] - 330s 10s/step - loss: 3.5397 - edit_distance: 0.1531\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5WyepEDwpUEc"
      },
      "source": [
        "## Predict function for inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HjWHfCAXpUEc"
      },
      "source": [
        "# model for predicting\n",
        "predict_model = Model(model.get_layer('sif_input').input, model.get_layer('output').output)\n",
        "\n",
        "class_map = {i: f\"-{uniq}\" for i, uniq in enumerate(unique_classes)}\n",
        "class_map[-1] = \"\"\n",
        "\n",
        "def Predict(spec, class_map):\n",
        "    sif = extract_SIF(spec).T\n",
        "    logits = predict_model(tf.expand_dims(sif, 0), training= False)\n",
        "    decoded_logits = tf.keras.backend.ctc_decode(logits, [tf.shape(logits)[1]], greedy=False)[0][0]\n",
        "    pred = \"\".join([class_map[i] for i in decoded_logits[0].numpy()]).strip(\"-\")\n",
        "    return pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JVZcy-R2UMQc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "159c6bd7-d78d-4903-9572-82b2f54f91b9"
      },
      "source": [
        "i=11 # example predict on a sample from test set\n",
        "print(\"pred: \", Predict(np.load(test_set[i][0]), class_map), \"\\ngndt: \", \"\".join([f\"{c} \" for c in test_set[i][3]]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "pred:  jackhammer-drilling-gun_shot-dog_bark-street_music \n",
            "gndt:  jackhammer drilling gun_shot dog_bark street_music \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7sBgS0YZkt4J"
      },
      "source": [
        "# Predict on all spectogram files in the 'test_feat_path' folder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rBJqUlpXqvqb"
      },
      "source": [
        "est_save_path = \"task2_labels_test.csv\"\r\n",
        "# est_save_path = \"cellar/temp/task2/est.csv\"\r\n",
        "\r\n",
        "test_feat_path= \"test_task2/feats\"\r\n",
        "# test_feat_path= \"#shared_train/sample_test_task2/feats\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bNbB6M3bqvqb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71f8d113-2c18-4255-d132-102471f771f4"
      },
      "source": [
        "# make the predictions and save as a csv file\n",
        "predictions = []\n",
        "class_map = {i: f\"-{uniq}\" for i, uniq in enumerate(unique_classes)}\n",
        "class_map[-1] = \"\"\n",
        "for f in os.listdir(test_feat_path):\n",
        "    spec = np.load(f\"{test_feat_path}/{f}\")\n",
        "    predictions.append([f.split('.')[0], Predict(spec, class_map)])\n",
        "if not os.path.exists(\"cellar/temp/task2\"): os.makedirs(\"cellar/temp/task2/\")\n",
        "np.savetxt(est_save_path, predictions, delimiter =\",\",  fmt ='%s') "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:201: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pWwrR4DT9aSD"
      },
      "source": [
        "# for running the score function from utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BNaXp4ASyrAc"
      },
      "source": [
        "# !pip install python-Levenshtein"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UBjJwVI_kmhZ"
      },
      "source": [
        "import utils # the utils.py given for evaluation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OsFpIa1qPUn-"
      },
      "source": [
        "ground_truth_labels_path = \"#shared_train/sample_test_task2/labels.csv\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-KFsPWXRqvqc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48232a70-eac4-4b7b-f5d4-27322d6816cb"
      },
      "source": [
        "print(\"score:\", utils.evals(ground_truth_labels_path, est_save_path, 2))\n",
        "p = utils.read_csv(est_save_path)\n",
        "t = utils.read_csv(ground_truth_labels_path)\n",
        "for k in sorted(p.keys()):\n",
        "    ed, sc = utils.editDistance(t[k], p[k])\n",
        "    print(f\"{k}:\", \"editD: \", ed, \"score: %.3f\"%sc, \"\\n\", \"\".join([\" \"]*len(f\"{k}\")), f\" pred: \", p[k], \"\\n\", \"\".join([\" \"]*len(f\"{k}\")), \" gndt: \", t[k])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "score: 0.825\n",
            "a00001: editD:  1 score: 0.750 \n",
            "         pred:  dog_bark-jackhammer-engine_idling \n",
            "         gndt:  siren-dog_bark-jackhammer-engine_idling\n",
            "a00002: editD:  1 score: 0.667 \n",
            "         pred:  car_horn-gun_shot \n",
            "         gndt:  car_horn-drilling-gun_shot\n",
            "a00003: editD:  1 score: 0.500 \n",
            "         pred:  drilling \n",
            "         gndt:  drilling-siren\n",
            "a00004: editD:  0 score: 1.000 \n",
            "         pred:  jackhammer-street_music-drilling-dog_bark \n",
            "         gndt:  jackhammer-street_music-drilling-dog_bark\n",
            "a00005: editD:  0 score: 1.000 \n",
            "         pred:  jackhammer-children_playing-street_music \n",
            "         gndt:  jackhammer-children_playing-street_music\n",
            "a00006: editD:  0 score: 1.000 \n",
            "         pred:  jackhammer \n",
            "         gndt:  jackhammer\n",
            "a00007: editD:  0 score: 1.000 \n",
            "         pred:  engine_idling-street_music \n",
            "         gndt:  engine_idling-street_music\n",
            "a00008: editD:  2 score: 0.333 \n",
            "         pred:  air_conditioner-dog_bark \n",
            "         gndt:  children_playing-car_horn-dog_bark\n",
            "a00009: editD:  0 score: 1.000 \n",
            "         pred:  engine_idling-jackhammer-car_horn \n",
            "         gndt:  engine_idling-jackhammer-car_horn\n",
            "a00010: editD:  0 score: 1.000 \n",
            "         pred:  dog_bark-jackhammer \n",
            "         gndt:  dog_bark-jackhammer\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pRBcQctqqvqc"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}